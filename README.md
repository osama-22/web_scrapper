# ğŸ¦¾ FastAPI Web Scraper  

This is a **FastAPI-based web scraper** designed to extract product data from an e-commerce website. The scraper collects **product names, prices, and images**, stores them efficiently, and notifies users about the scraping process.  

The project is **modular and extensible**, allowing easy integration with **different storage backends (JSON, Database, Cloud)** and **notification systems (Console, Email, Webhooks)**.

---

## ğŸš€ Features  
âœ… **Scrapes product data** (name, price, image)  
âœ… **Configurable settings**:
   - Limit the number of pages to scrape  
   - Use a proxy for requests  
âœ… **Storage Options**:
   - JSON (default)  
   - Easily extendable to Databases like PostgreSQL or MongoDB  
âœ… **Notification Options**:
   - Console (default)  
   - Extendable to Email, Webhooks, Slack, etc.  
âœ… **Error Handling & Retries**:
   - Retries failed requests (`RETRY_COUNT` configurable)  
âœ… **Caching with Redis**:
   - Prevents redundant updates when the product price hasnâ€™t changed  
âœ… **Authentication**:
   - Simple token-based authentication  

---

## ğŸ“‚ Project Structure  

```bash
/scraper_app
â”‚â”€â”€ main.py                 # FastAPI entry point
â”‚â”€â”€ config.py               # Loads .env variables
â”‚â”€â”€ storage/                # Storage-related files
â”‚   â”‚â”€â”€ base.py             # Abstract Base Class for Storage
â”‚   â”‚â”€â”€ json_storage.py     # JSON Storage Implementation
â”‚   â”‚â”€â”€ db_storage.py       # Database Storage (Placeholder)
â”‚â”€â”€ notifier/               # Notification-related files
â”‚   â”‚â”€â”€ base.py             # Abstract Base Class for Notifier
â”‚   â”‚â”€â”€ console_notifier.py # Console Notifier Implementation
â”‚   â”‚â”€â”€ email_notifier.py   # Email Notifier Implementation (Example)
â”‚â”€â”€ scraper.py              # Web Scraper Class
â”‚â”€â”€ auth.py                 # Authentication Handler
â”‚â”€â”€ cache.py                # Redis Caching
â”‚â”€â”€ models.py               # Pydantic Models
â”‚â”€â”€ requirements.txt        # Dependencies
â”‚â”€â”€ .env.example            # Example environment variables
â”‚â”€â”€ README.md               # Documentation
```

---

# ğŸ› ï¸ Setup Instructions  

Follow these steps to set up and run the FastAPI web scraper.  
## **1ï¸âƒ£ Clone the Repository**  
First, clone the repository from GitHub and navigate into the project directory:  
```sh
git clone https://github.com/osama-22/web_scrapper.git
cd web_scrapper
```

## **2ï¸âƒ£ Create & Activate a Virtual Environment**  
```sh
python -m venv venv
source venv/bin/activate
```

## **3ï¸âƒ£ Install Dependencies**  
Install the required Python packages using pip:  
```sh
pip install -r requirements.txt
```

## **4ï¸âƒ£ Set Up Environment Variables**  
Copy the `.env.example` file and create a new `.env` file:  
```sh
cp .env.example .env
```
Edit the `.env` file with your specific settings.
```sh
API_KEY=your_static_api_key
REDIS_HOST=localhost
REDIS_PORT=6379
RETRY_COUNT=3
```

## **5ï¸âƒ£ Start Redis Server (For Caching)**  
For macOS (Homebrew):
```sh
brew install redis
brew services start redis
```
For Ubuntu/Debian:
```sh
sudo apt update
sudo apt install redis
sudo systemctl start redis
sudo systemctl enable redis
```
To verify Redis is running, open a new terminal window and run:
```sh
redis-cli ping
```
Expected output:
```sh
PONG
```

## **6ï¸âƒ£ Run the Scraper**  
Start the FastAPI server:  
```sh
uvicorn main:app --reload
```
The server should start and display:
```sh
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
```

## **7ï¸âƒ£ Test the API**  
### Using Postman:
1. Open Postman and create a new request.
2. Set the request type to POST.
3. Enter the URL: 
```sh
http://127.0.0.1:8000/scrape/
```
4. Go to the Headers tab and add:
```sh
Key: Authorization
Value: your_static_api_key
```
5. Go to the Body tab, select raw, choose JSON, and enter:
```sh
{
  "pages": 1,
  "proxy": ""
}
```
6. Click "Send".

### Using cURL
Alternatively, test the API using cURL in the terminal:
```sh
curl -X POST "http://127.0.0.1:8000/scrape/" \
     -H "Content-Type: application/json" \
     -H "Authorization: your_static_api_key" \
     -d '{"pages": 1, "proxy": ""}' 
```

---


